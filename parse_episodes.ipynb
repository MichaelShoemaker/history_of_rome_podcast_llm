{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376f82e2",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.split('[')[0])  # Handle packages with extras like torch[cuda]\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages for transcription\n",
    "packages = ['faster-whisper', 'torch', 'torchaudio']\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gary/history_of_rome_epidode_search/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Using CPU mode to avoid GPU crashes\n",
      "This will be slower but more stable\n",
      "\n",
      "Loading Whisper model: medium\n",
      "Device: cpu, Compute type: int8\n",
      "✓ Model loaded successfully!\n",
      "\n",
      "Found 192 MP3 files to transcribe\n",
      "First few files:\n",
      "  1. 20070728 - 001- In the Beginning.mp3\n",
      "  2. 20100225 - 002- Youthful Indiscretions.mp3\n",
      "  3. 20100225 - 003a- The Seven Kings of Rome.mp3\n",
      "  4. 20100225 - 003b- The Seven Kings of Rome.mp3\n",
      "  5. 20100225 - 004- The Public Thing.mp3\n",
      "  ... and 187 more files\n",
      "\n",
      "⏱️ CPU Performance estimate: ~0.5-1x real-time (30min episode = 30-60min)\n",
      "Total estimated time for all episodes: 384.0 hours\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "EPISODES_DIR = \"history_of_rome_episodes\"  # Directory with MP3 files\n",
    "TRANSCRIPTS_DIR = \"all_transcripts\"    # Directory to save transcripts\n",
    "MODEL_SIZE = \"medium\"  # Options: tiny, base, small, medium, large-v2, large-v3\n",
    "\n",
    "# Create transcripts directory\n",
    "os.makedirs(TRANSCRIPTS_DIR, exist_ok=True)\n",
    "\n",
    "# Check CUDA availability and clear cache\n",
    "def setup_device():\n",
    "    \"\"\"Setup the best available device for transcription\"\"\"\n",
    "    # Force CPU mode to avoid GPU crashes\n",
    "    print(\"🔧 Using CPU mode to avoid GPU crashes\")\n",
    "    print(\"This will be slower but more stable\")\n",
    "    return \"cpu\", \"int8\"\n",
    "    \n",
    "    # Original GPU detection code (commented out for stability)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     print(f\"CUDA is available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    #     print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    #     \n",
    "    #     # Clear CUDA cache to free up memory\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     \n",
    "    #     try:\n",
    "    #         # Test CUDA with a simple operation\n",
    "    #         test_tensor = torch.tensor([1.0]).cuda()\n",
    "    #         del test_tensor\n",
    "    #         torch.cuda.empty_cache()\n",
    "    #         \n",
    "    #         print(\"CUDA test successful - using GPU\")\n",
    "    #         return \"cuda\", \"float16\"\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"CUDA test failed: {e}\")\n",
    "    #         print(\"Falling back to CPU\")\n",
    "    #         return \"cpu\", \"int8\"\n",
    "    # else:\n",
    "    #     print(\"CUDA not available - using CPU\")\n",
    "    #     return \"cpu\", \"int8\"\n",
    "\n",
    "# Setup device\n",
    "DEVICE, COMPUTE_TYPE = setup_device()\n",
    "\n",
    "# Initialize the Whisper model with error handling\n",
    "print(f\"\\nLoading Whisper model: {MODEL_SIZE}\")\n",
    "print(f\"Device: {DEVICE}, Compute type: {COMPUTE_TYPE}\")\n",
    "\n",
    "try:\n",
    "    model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE_TYPE)\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load model with {DEVICE}: {e}\")\n",
    "    print(\"Trying with CPU fallback...\")\n",
    "    \n",
    "    try:\n",
    "        model = WhisperModel(MODEL_SIZE, device=\"cpu\", compute_type=\"int8\")\n",
    "        DEVICE = \"cpu\"\n",
    "        COMPUTE_TYPE = \"int8\"\n",
    "        print(\"✓ Model loaded successfully on CPU!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"✗ Failed to load model on CPU: {e2}\")\n",
    "        print(\"Trying with smaller model...\")\n",
    "        model = WhisperModel(\"small\", device=\"cpu\", compute_type=\"int8\")\n",
    "        MODEL_SIZE = \"small\"\n",
    "        DEVICE = \"cpu\"\n",
    "        COMPUTE_TYPE = \"int8\"\n",
    "        print(\"✓ Small model loaded successfully on CPU!\")\n",
    "\n",
    "# Find all MP3 files\n",
    "mp3_files = glob.glob(os.path.join(EPISODES_DIR, \"*.mp3\"))\n",
    "mp3_files.sort()  # Sort to process in order\n",
    "\n",
    "print(f\"\\nFound {len(mp3_files)} MP3 files to transcribe\")\n",
    "\n",
    "if len(mp3_files) == 0:\n",
    "    print(f\"No MP3 files found in {EPISODES_DIR} directory!\")\n",
    "    print(\"Make sure you've downloaded the episodes first using the pull_episodes.ipynb notebook.\")\n",
    "else:\n",
    "    print(\"First few files:\")\n",
    "    for i, file in enumerate(mp3_files[:5]):\n",
    "        print(f\"  {i+1}. {os.path.basename(file)}\")\n",
    "    if len(mp3_files) > 5:\n",
    "        print(f\"  ... and {len(mp3_files) - 5} more files\")\n",
    "\n",
    "# Performance estimates\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"\\n🚀 GPU Performance estimate: ~2-4x real-time (30min episode = 7-15min)\")\n",
    "else:\n",
    "    print(f\"\\n⏱️ CPU Performance estimate: ~0.5-1x real-time (30min episode = 30-60min)\")\n",
    "    \n",
    "print(f\"Total estimated time for all episodes: {len(mp3_files) * 0.5 if DEVICE == 'cuda' else len(mp3_files) * 2:.1f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting transcription of 192 episodes...\n",
      "======================================================================\n",
      "[1/192] Skipping 20070728 - 001- In the Beginning - transcript already exists\n",
      "[2/192] Skipping 20100225 - 002- Youthful Indiscretions - transcript already exists\n",
      "\n",
      "[3/192] Processing: 20100225 - 003a- The Seven Kings of Rome\n",
      "Transcribing: 20100225 - 003a- The Seven Kings of Rome.mp3\n"
     ]
    }
   ],
   "source": [
    "def transcribe_episode(audio_file_path, output_file_path):\n",
    "    \"\"\"Transcribe a single episode and save with timestamps\"\"\"\n",
    "    try:\n",
    "        print(f\"Transcribing: {os.path.basename(audio_file_path)}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simplified transcription settings to avoid crashes\n",
    "        segments, info = model.transcribe(\n",
    "            audio_file_path, \n",
    "            beam_size=3,  # Reduced beam size for stability\n",
    "            word_timestamps=True,\n",
    "            language=\"en\",  # Force English for History of Rome\n",
    "            # Removed VAD filter as it might cause issues\n",
    "        )\n",
    "        \n",
    "        # Write results with timestamps\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Write header with episode info\n",
    "            episode_name = os.path.basename(audio_file_path).replace('.mp3', '')\n",
    "            f.write(f\"# {episode_name}\\n\")\n",
    "            f.write(f\"# Detected language: {info.language}\\n\")\n",
    "            f.write(f\"# Duration: {info.duration:.2f} seconds\\n\")\n",
    "            f.write(f\"# Model: {MODEL_SIZE}, Device: {DEVICE}\\n\\n\")\n",
    "            \n",
    "            # Write timestamped segments\n",
    "            segment_count = 0\n",
    "            for segment in segments:\n",
    "                # Format timestamps as [MM:SS --> MM:SS]\n",
    "                start_min, start_sec = divmod(int(segment.start), 60)\n",
    "                end_min, end_sec = divmod(int(segment.end), 60)\n",
    "                \n",
    "                timestamp = f\"[{start_min:02d}:{start_sec:02d} --> {end_min:02d}:{end_sec:02d}]\"\n",
    "                line = f\"{timestamp} {segment.text.strip()}\"\n",
    "                f.write(line + \"\\n\")\n",
    "                segment_count += 1\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        file_size = os.path.getsize(output_file_path) / 1024  # KB\n",
    "        print(f\"✓ Completed in {elapsed_time:.1f}s: {os.path.basename(output_file_path)} ({file_size:.1f} KB, {segment_count} segments)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to transcribe {os.path.basename(audio_file_path)}: {str(e)}\")\n",
    "        # Remove partial file if it exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            os.remove(output_file_path)\n",
    "        return False\n",
    "\n",
    "# Process all episodes\n",
    "if len(mp3_files) > 0:\n",
    "    print(f\"\\nStarting transcription of {len(mp3_files)} episodes...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    successful_transcriptions = 0\n",
    "    failed_transcriptions = 0\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, mp3_file in enumerate(mp3_files, 1):\n",
    "        # Create output filename\n",
    "        episode_name = os.path.basename(mp3_file).replace('.mp3', '')\n",
    "        output_file = os.path.join(TRANSCRIPTS_DIR, f\"{episode_name}.txt\")\n",
    "        \n",
    "        # Skip if transcript already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"[{i}/{len(mp3_files)}] Skipping {episode_name} - transcript already exists\")\n",
    "            successful_transcriptions += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(mp3_files)}] Processing: {episode_name}\")\n",
    "        \n",
    "        if transcribe_episode(mp3_file, output_file):\n",
    "            successful_transcriptions += 1\n",
    "        else:\n",
    "            failed_transcriptions += 1\n",
    "        \n",
    "        # Small delay between files\n",
    "        time.sleep(1)\n",
    "    \n",
    "    total_elapsed = time.time() - total_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Transcription Summary:\")\n",
    "    print(f\"✓ Successful: {successful_transcriptions}\")\n",
    "    print(f\"✗ Failed: {failed_transcriptions}\")\n",
    "    print(f\"⏱️ Total time: {total_elapsed/60:.1f} minutes\")\n",
    "    print(f\"📁 Transcripts saved to: {os.path.abspath(TRANSCRIPTS_DIR)}\")\n",
    "    \n",
    "    # List some completed transcripts\n",
    "    transcript_files = [f for f in os.listdir(TRANSCRIPTS_DIR) if f.endswith('.txt')]\n",
    "    print(f\"\\nCompleted transcripts ({len(transcript_files)} files):\")\n",
    "    for file in sorted(transcript_files)[:10]:  # Show first 10\n",
    "        file_path = os.path.join(TRANSCRIPTS_DIR, file)\n",
    "        file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "        print(f\"  • {file} ({file_size:.1f} KB)\")\n",
    "    if len(transcript_files) > 10:\n",
    "        print(f\"  ... and {len(transcript_files) - 10} more files\")\n",
    "else:\n",
    "    print(\"No MP3 files to process!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce29a206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 GPU Troubleshooting Tools\n",
      "========================================\n",
      "Current GPU status:\n",
      "Sun Sep 28 11:24:38 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050        Off | 00000000:08:00.0  On |                  N/A |\n",
      "|  0%   44C    P3              20W / 130W |    281MiB /  8192MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1656      G   /usr/lib/xorg/Xorg                          129MiB |\n",
      "|    0   N/A  N/A      2993      G   ...erProcess --variations-seed-version      143MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "========================================\n",
      "If you're getting CUDA errors, you can:\n",
      "1. Restart your Jupyter kernel (Kernel -> Restart)\n",
      "2. Run kill_gpu_processes() to free up GPU memory\n",
      "3. The script will automatically fall back to CPU if needed\n"
     ]
    }
   ],
   "source": [
    "# GPU Troubleshooting and Process Management\n",
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "def check_gpu_processes():\n",
    "    \"\"\"Check what processes are using the GPU\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"Current GPU status:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"nvidia-smi not available or failed\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"nvidia-smi not found - NVIDIA drivers may not be installed\")\n",
    "\n",
    "def kill_gpu_processes():\n",
    "    \"\"\"Kill processes that might be hogging the GPU\"\"\"\n",
    "    gpu_processes = []\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            # Look for common GPU-using processes\n",
    "            if proc.info['name'] in ['python', 'jupyter-lab', 'jupyter', 'code']:\n",
    "                cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "                if any(keyword in cmdline.lower() for keyword in ['torch', 'cuda', 'gpu', 'whisper', 'ml', 'tensorflow']):\n",
    "                    gpu_processes.append(proc)\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            continue\n",
    "    \n",
    "    if gpu_processes:\n",
    "        print(f\"Found {len(gpu_processes)} potential GPU processes:\")\n",
    "        for proc in gpu_processes:\n",
    "            print(f\"  PID {proc.info['pid']}: {proc.info['name']}\")\n",
    "        \n",
    "        response = input(\"Kill these processes? (y/N): \").lower()\n",
    "        if response == 'y':\n",
    "            for proc in gpu_processes:\n",
    "                try:\n",
    "                    proc.terminate()\n",
    "                    print(f\"Terminated PID {proc.info['pid']}\")\n",
    "                except:\n",
    "                    print(f\"Failed to terminate PID {proc.info['pid']}\")\n",
    "    else:\n",
    "        print(\"No obvious GPU processes found\")\n",
    "\n",
    "print(\"🔧 GPU Troubleshooting Tools\")\n",
    "print(\"=\" * 40)\n",
    "check_gpu_processes()\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"If you're getting CUDA errors, you can:\")\n",
    "print(\"1. Restart your Jupyter kernel (Kernel -> Restart)\")\n",
    "print(\"2. Run kill_gpu_processes() to free up GPU memory\")\n",
    "print(\"3. The script will automatically fall back to CPU if needed\")\n",
    "\n",
    "# Uncomment the next line if you want to automatically kill GPU processes\n",
    "# kill_gpu_processes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcad4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single episode first to make sure everything works\n",
    "print(\"🧪 Testing with a single episode first...\")\n",
    "\n",
    "if len(mp3_files) > 0:\n",
    "    test_file = mp3_files[0]  # First episode\n",
    "    test_output = os.path.join(TRANSCRIPTS_DIR, f\"TEST_{os.path.basename(test_file).replace('.mp3', '.txt')}\")\n",
    "    \n",
    "    print(f\"Testing with: {os.path.basename(test_file)}\")\n",
    "    \n",
    "    if transcribe_episode(test_file, test_output):\n",
    "        print(\"✅ Test successful! You can now run the full batch.\")\n",
    "        \n",
    "        # Show a sample of the transcript\n",
    "        if os.path.exists(test_output):\n",
    "            with open(test_output, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                print(\"\\n📄 Sample transcript:\")\n",
    "                for line in lines[:10]:  # Show first 10 lines\n",
    "                    print(f\"  {line.strip()}\")\n",
    "                if len(lines) > 10:\n",
    "                    print(f\"  ... and {len(lines) - 10} more lines\")\n",
    "    else:\n",
    "        print(\"❌ Test failed. Check the error messages above.\")\n",
    "else:\n",
    "    print(\"No MP3 files found for testing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d10c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
